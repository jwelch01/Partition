\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{setspace}

\title{Improved Ranking through Black Box Testing}
\author{Jesse Welch}
\date{}

\doublespace
\begin{document}
\maketitle



\section{Introduction}

\section{Ranking Solutions}
\subsection*{The Algorithm}
When presented with the black box test results of many solutions to the same problem, one of the simplest and most useful things to do is to rank the functional correctness of the solutions. Naive algorithms for this perform poorly. For example, ranking solutions based on the percentage of tests that pass is extremely biased, frequently ranking solutions with a single, easy to test flaw as less correct than solutions with multiple, difficult to test bugs.

Claessen et. al. demonstrated an algorithm that is a substantial improvement \cite{Claessen}. They created large test suites automatically using QuickCheck \cite{QuickCheck}, and as a result, most of their tests were entirely equivalent. Rather than considering each test result as equally informative and comparing based on the results of every test run, they partitioned the tests into equivalence classes.

\centerline{ Given tests $T_O$ and $T_1$ and solution set $S$}
$$ T_0 \equiv T_1 \iff \forall s \in S s(T_0) \equiv s(T_1) $$

Using this example, a test set $T$ could be reduced into a set of its equivalence classes. From these equivalence classes, a single representative example could be drawn, creating a new test set $T\prime$. This step alone substantially improves the validity of the ranking algorithm. Ranking students based on the percentage of tests passed in $T\prime$, a solution with a single flaw will rank higher than a program with multiple flaws, regardless of how easy they are to test.

However, this method is still limited, and still has its own biases. For example, in some solutions, a single flaw will cause the failure of multiple equivalence classes of tests, causing the flaw to look worse than a seemingly equivalent flaw in a different program. And in general, we are still create a direct ranking of solutions whose failures may come in completely different sections of the problem, and thus be truthfully incomparable.

This general flaw is a consequence of the fact that a true ranking algorithm creates a total order of the solutions. When utilizing a percentage based metric, for every pair of solutions, one must be more correct, or they are equivalent. However, sometimes solutions are truly incomparable. For example, a problem may contain two subproblems, each of them entirely distinct. If one solution fails tests only on the first subproblem, and a second solution fails only on the second subproblem, they are truly incomparable. Claessen's algorithm allows for a "ranking" that represents this fact. In lieu of an artificial total order of solutions, Claessen's algorithm creates a partial order.

\centerline{Given solutions $S_1$ and $S_2$, and test set $T$}
$$S_1 \equiv S_2 \iff \forall t \in T S_1(t) \equiv S_2(t)$$
$$S_1 > S_2 \iff \forall t \in T S_1(t) \geq S_2(t) \wedge \exists t \in T \Rightarrow S_1(t) \neq S_2(t)$$

Under this definition, there are 4 possible results for comparing two solutions, $S_1$ and $S_2$. $S_1 < S_2$, $S_1 > S_2$, $S_1 = S_2$, or $S_1$ and $S_2$  are incomparable. This definition gives us every possible relationship between the solutions, rather than the 3 possibilities presented by a percentage based ranking. Additionally, it removes any biases based on the \emph{number} of tests failed. Instead, it is based only on the relationship between failed tests - anomalous failure counts will not affect the ranking.

Using this algorithm, we are able to successfully create accurate rankings of different student solutions.

(insert two or three diagrams graphs, brief description of how the graph works)
\subsection*{Adding More Results}
\subsection*{Limiting the Test Set}

\section{The Implication Graph}

\section{Noise Reduction on Test Set}
\subsection*{Claessen's Algorithm}
In Claessen's paper on ranking, there are several forms of noise reduction already performed. The first is the fundamental step of the algorithm, in which tests are grouped into equivalence classes. This step alone reduces from the original number of tests to a substantially smaller subset. Even with test sets numbering well into the thousands, this step never produced more than 144 equivalence classes during our experimentation.

After this large scale reduction, Claessen's algorithm contains another form of noise reduction. A test $T_0$ is redundant if

$$FAIL (T_0) \equiv FAIL(T_1) \cup FAIL(T_2)$$
where all three tests are drawn from different equivalence classes. This was justified with the argument that, rather than discovering a new faults of its own, $T_0$ likely provoked both of the faults discovered in $T_1$ and $T_2$. Removing $T_0$ therefore does not cause any information loss. Instead, its removal simplifies the test set to a smaller, but still representative sample of tests.

(if I have time, insert a theorem here about how removing this test also WILL NOT affect the ranking)

(a little bit of data about how much this generally reduces)

\subsection*{Union*}
The logical basis to Claessen's algorithm that a test can be redundant because it provokes the flaws already demonstrated by two other non-redundant tests\cite{Claessen}, can be extended further. Why should we limit this to only circumstances where the flaws are in exactly two other tests? Why not any number? We propose an extended algorithm, one which removes any tests whose set of failures is equal to the union of the failures of \emph{any number of tests}. 

A test $T_0$ is redundant if

$$FAIL (T_0) \equiv FAIL(T_1) \cup FAIL(T_2) \cup ... \cup FAIL(T_n) $$

(if I managed to write the first proof, proving this the same shouldn't be hard)

A naive implementation of this algorithm is computationally complex. Given a set of tests $T$, we need to run the algorithm for each member $t$ of $T$, checking whether its failure set is equal to the union of any subset of tests in $T$ that does not contain $t$. This is simply too much work, so a different solution must be found.

Thankfully, the implication graph lends itself to a vastly simplified implementation. We do not need to check the union of every possible subset of tests, only the union of those that could possibly be useful. Using the implication graph, given a test $t$ and a test set $T$, we can easily find the subset $T\prime$ which only contains tests whose failures are a subset of $t$. Any test in $T$ but not in $T\prime$ cannot possibly contribute to an equal union, because it contains at least one failure not in $t$. Therefore, we can simply apply the algorithm once, to every member of $T\prime$. $t$ is redundant if $FAIL(t) \equiv \cup^* T\prime$.

\subsection*{Comparison}
Union* must reduce at least as many tests as Claessen, and can reduce more. This is because every test that will be removed under Claessen will be removed under exactly the same conditions in Union* (the union of two tests is simply a specification of the union of any number of tests). In theory, Union* is capable of outperforming Claessen

(diagram of toy example where it succeeds)

In practice, we discovered a different situation. On 10 different real world datasets (each dataset was the student submissions for a different assignment, drawn from two different courses), we found no difference between reduction performed by the Claessen algorithm and the Union* algorithm. This was a surprising result, and demonstrates several things. Trivially, it confirms that the Union* algorithm does no worse than Claessen. More substantially, we discovered that the Claessen algorithm is surprisingly powerful. In every tested real world case, the union of two tests was enough to remove every test that could be found redundant under union.

(side by side of the two identical reductions)

There are several potential explanations for this fact. One is that no test tests more than two underlying faults. This is extremely unlikely, as it is relatively simple to write a test that exercises large sections of a program's functionality, and can readily provoke more than two potential faults. Another possibility lies in the method of building tests. The test sets we use, being generally developed from QuickCheck properties, deliberately exercise a specific area of functionality \cite{QuickCheck}. Similarly, when hand written unit tests are allowed, they also generally exercise small areas of functionality, rather than the entire program. A different style of test design might allow Union* to have more success, removing large tests that attempt to exercise every possible fault in the program, when these faults are better demonstrated by small tests. A final possibility is that we simply don't have enough solutions, and that a greater diversity in solutions would increase the difference between the two algorithms.

\section{Noise Reduction on Witness Set}

There exists a minimal subset of tests which we must retain in order for the ranking algorithm to continue to function without information loss. This set of tests is not necessarily as small as the minimal number of tests needed to diagnose the failures in a single solution. There exist many situations in which multiple tests are needed to differentiate between the entire set of solutions, but a single representative test would be enough to diagnose the flaws of an individual solution.

All forms of noise reduction previously discussed rely on examining the relationships between the classes of solutions, and removing tests on which all members of a  class can be expected to perform the same. However, when attempting noise reduction at the granularity of a single solution, the relationships between solutions are not applicable. The only relationships relevant at this level  are those between the tests themselves.

The implication graph is a representation of the relationships between these tests. It presents many discoverable relationships (implies, implied by, reachable from, etc.). Using this graph, we are able to perform noise reduction at the level of a single solution, a novel process. For simplicity, we chose to examine only one of the available relationships, direct implication. Our algorithm is as follows:

(for two tests, T1, T2, if T1(S) = FAIL \&\& T2(S) = FAIL \&\& FAIL(T1) -> FAIL(T2), remove T2 from S)

Given a perfect implication graph, in which every implication is not just found in the data but representative of the problem itself, this algorithm reduces the tests presented as  failed to the set of the easiest tests failed. 

If two tests are needed to differentiate the solutions on the solution class scale, it is possible (and frequently true), that one is simply an easier version of itself. For exaxmple, we ran our algorithms on a set of solutions to the problem of reducing the lambda calculus. Following all class level reductions, two tests remained:

(the tests (numbered))

(a sentence or two on what each test tested,  and why the first implied the second)

Therefore, if a solution failed test 1, failing test 2 tells us nothing new. They may have the flaw it was attempting to diagnose, or they may not. This fact is masked entirely by the flaw diagnosed in test 1. Therefore, removing test 2 from the witnesses would not cause any information loss.

Because the flaw in test 1 directly causes a failure in test 2, FAIL(Test1) -> FAIL(Test2). This led our algorithm to be able to reduce the witnesses to any solution who failed both tests, providing only the output from test 1.

(figure showing that this happened, probably a side by side).

(numbers on how avg number and percentage of reductions performed)

This algorithm is definitively  capable of performing reductions, but a questions remains: Are the reductions good? Reductions performed on the class level cannot change the class structure. Both the Claessen algorithm and the Union* algorithm cannot cause information loss. But the witness reductions can. Though implications have a theoretical underlyin structure, they are drawn from the underlying data available, and this data is frequently flawed. This can lead to reductions in a test can be removed based on an implication from a test which tested an entirely different separate feature.

A readily available example is the dataset used to confirm the implication graph. As we discovered earlier, when tests were run on functions whose implementations were entirely independent, the functions generally formed into groups in the  graph, but there were implications between the groups. Using this implication graph, we performed a witness reduction on each solution, in order to test how many of the reductions we considered to be valid. In order to simplify analysis, we did not try ot delve into the relationship between each test, but instead used a simple metric. If two tests test the same function, any reduction upon them is valid. If they test different  functions, any reduction on them is invalid.

(figure showing side by side several good reductions, and several bad ones)

(analysis, percentage of what reductions were good)

The invalid reductions discovered represent a failure in the implication graph, as they are based on implications which are likely not valid to the problem itself, but simply to the dataset the reduction process was done on. If the implication graph perfectly represented the problem structure, every reduction based on it would be good. Therefore, the more correct we make the implication graph, the more correct we make the witness reduction.



\section{Improving the Implication Graph}



When building the implication graph, we do not have access to the structure of the problem itself, only a small sample of attempts to solve the problem and there results on tests of the problem. We have no oracle to tell us when the structure is correct. However, there are many properties about the relationships between tests that we can utilize to remove implications that cannot possibly be present in the problem itself, as well as removing implications that may be true, but the we do not have enough information  to claim.

One class of implications we can remove entirely are those between the failure of one test, and the passage of another. Every problem has a fully correct and a fully incorrect solution, one which passes every test, and one which fails every test. Based on this, the following proofs are possible.


Assumption: There exists a fully correct and fully incorrect solution to every problem.

Theorem: The passage of one test cannot imply the failure of another test in the full problem space.

Proof: Assume PASS(T1) -> FAIL(T2). This means that every solution that passes T1 fails T2. However, there exists the fully correct solution S. For all tests T, S(T) = PASS. Therefore S(T1) = PASS, and S(T2) = PASS, contradicting our assumption.

Theorem: The failure of one test cannot imply the failure of another test in the full problem space.

Proof: Assume FAIL(T1) -> PASS(T2). This means that every solution that fails T1 passess T2. However, there exists the fully incorrect solution S'. For all tests T, S'(T) = FAIL. Therefore S'(T1) = FAIL, and S'(T2) = FAIL, contradicting our assumption.


Based on these proofs, we can safely remove from the implication graph every implication between the passage of one test and the failure of another.

From similar logic, we can remove every implication either to or from a test with universal results. If every student passed a given test, then any result on any other test will logically imply this passage. However, there exists the fully incorrect solution, which by definition failed this test. Therefore these implications are by definition based on incomplete information, because they do not take into account the failure of this test. The inverse means we can do the same to a test which every student failed, based on the fully correct solution. Similarly, if no student passed a given test, the  test's passage will logically imply everything. But we cannot make implications  from this result (or a test with no failures) because the fully correct and incorrect solutions mean we are basing these implications on incomplete data.

There is one other reduction we can make to the implication graph, for somewhat different reasons than the previous reductions. We can remove implications that may hold true in th actual problem structure, because we do not currently have enough information to assume them. If a given test result is achieved only by members of a single equivalence class, any implications based on it can be removed, because they are based on too little information to generalize from.

(insert a paragraph on "know one thing know them all". Try and  find a tone that gets clarity but isn't overly colloquial)

\subsection*{The Results  of Reducing the Implication Graph}

These reductions of the implication graph were critical to our ability to implement witness reduction. The first several reductions insured we were not making any reductions based on clearly invalid implications. The final reduction, in which we did not allow one fact about a solution to imply everything about it, was particularly important. Based on this reduction, we were able to give larger and more representative witness sets to the affected solutions.

(side by side of Ahnie's unique failure)

\subsection*{Continuing Problems}

Despite these reductions, we were not able to fully stop the algorithm from making invalid reductions. Though the implication graph can assuredly be improved further, and the witness reductions therefore improved as well, it is unlikely that we will ever be able to make completely valid reductions for most problems. The reason for this is twofold: the number of solutions available is finite, and  real world solutions are strange.

The first problem is fairly clear. In order to find the full structure of a problem, we need to know every possible correct and incorrect solution to it, an infinite  number of solutions. Presented with a finite dataset, we can only approximate this structure, a fundamental limitation.

The second problem is far more insidious. When attempting to find the structure of a problem, it is expected that if a test tests some feature of a program, and that feature has a flaw, the test will uncover it. However, in the real world, programs do not behave nicely. There may be two flaws within a feature, which generally cause the feature to fail, but sometimes work together to cause it to succeed.

(addition/multiplication example)

In real world programs, we do not find clean connections of components, failures that always occur in concert, features bein implemented in uniform ways. Real programmers have flaws present at every known edge case, and sometimes at edge cases that were unknown. Flaws interact in unexpected ways, and programs, when tested as a black box, are generally unpredictable.

Nonetheless, we have found that, on real world datasets, witness reduction based on our best approximation of the implication graph is good enough to produce positive results. Though the eccentricities of real programs often meant that reductions the seemed obvious were not made, and sometimes that invalid reductions were, the process generally proved successful, and should prove more successful as future improvements to the graph are made.

\subsection*{Witness Expansion}

The end goal of witness reduction is providing the smallest possible set of tests that diagnose the faults of a solution. A related goal is providing a solution the most accurate diagnostic information possible. The implication graph can be used to achieve this goal by expanding the witness set, rather than reducing it.

When we believed two tests were related based on the implication graph, we removed the harder test, because the simpler one provided the diagnostic information. However, for a more successful solution, which passed the easier test but failed the harder one, we provide no evidence of this relationship. But this relationship is a very useful thing, because it provides increased specificity on exactly where the fault lies within the solution. Using a very similar formula to our witness reduction, we can provide the student with these relationships.

(for two tests, T1, T2, if T1(S) = PASS \&\& T2(S) = FAIL \&\& FAIL(T1) -> FAIL(T2), inform of the relationship between T1 \&\& T2)





\section{Future Work}

\end{document}
