\documentclass[11pt,twoside]{article}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[letterpaper,inner=1.375in, outer=1.125in, top=1.25in, bottom=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{array}
\usepackage{tabularx}
\usepackage{verbatim}

\newcommand\dnr{\mathit{DNR}}
\newcommand\fail{\mathit{FAIL}}
\newcommand\pass{\mathit{PASS}}
\newcommand\defined{\triangleq}

\newtheorem{thm}{Theorem}

\let\cite=\citep
\bibpunct();A{},




\begin{document}

\title{Finding the Signal: Improved Ranking and Noise Reduction Through Black Box Testing}
\author{Jesse Welch}
\date{May 2012}

\pagestyle{headings}


\maketitle

\tableofcontents
\newpage

\baselineskip=1.2\baselineskip % a little extra space between lines


\section{Introduction}
Black-box testing sets are useful for many different people. They allow a programmer or researcher to diagnose faults in a program, without having to read the source code. Using black-box testing results, a set of failures can be presented without any detailed knowledge required.

When a set of black-box tests is run on a single solution to a problem, information about this solution is avalaible. When the tests are run on multiple solutions to a single problem, more information is available. When we add more solutions, we gain the ability to compare test results. We can use this ability to, compare solutions, rank the quality of solutions, find redundant tests, and generally provide more information, with less noise, to each solution.

Prior work demonstrates methods for both ranking and reducing redundant tests. In this paper, we improve upon these methods in several ways. We demonstrate a method for improving the ranking function by allowing additional results. We introduce a method for allowing the ranking algorithm to continue to function with incomplete results. We introduce a novel algorithm for performing additional noise reduction, limiting the set of failed tests shown to each individual solution. Finally, we introduce an infrastructure designed to make further algorithmic improvements and reduction strategies simple to implement.

\section{Ranking Solutions}
\subsection{What is Ranking?}
When we have multiple solutions to the same problem, we are generally trying to compare them in some way. Prior work has needed to compare various programming methodologies \cite{Claessen}. We have multiple outcomes because we are attempting to grade student homework assignments. The ability to compare the functional correctness of each submission helps us to group students into grade ranges and insure fairness in the grading.

"Fairness" is a particularly difficult concept to quantify, but it a fair ranking algorithm is necessary to be able to accurately compare solutions. A fair algorithm produces an objective comparison, in which the two solutions are correctly ordered by their functional correctness, regardless of the actions of an adversarial tester.

A ranking algorithm can therefore be improved by making it more fair. This can mean any of several things. If a new algorithm is able to find an objective relationship between two types of solutions considered equivalent by the original algorithm, the new algorithm is an improvement. Similarly, if a new algorithm discovers that two solutions are incomparable when the original algorithm found a relationship between them, the new algorithm is an improvement, because it has removed a relationship that did not truly exist.

Finding a fair ranking algorithm is exceedingly important for our task of grading student homework. Prior to implementing the algorithms below, our students were given feedback that listed their result on every test in our test sets, which were generally to large to be easily comprehended. 

Additionally, raw percentage of tests passed is not a fair metric by our definition. Consider two solutions, one with a bug in feature $F_1$, the other with a bug in $F_2$. An adversarial tester could easily choose which solution ranked superior by altering the number of tests run on each feature. If more tests are run on $F_1$, $F_2$ will be superior. If more tests are run on $F_2$, $F_1$ will be superior. 

Until we found an alternate algorithm, students could not be fairly compared algorithmically, and needed to be ranked based on the instructor's expert knowledge.

\subsection{The Algorithm}
Claessen et. al. demonstrated an algorithm for creating a fair ranking of solutions\cite{Claessen}. Their algorithm was run on large test suites generated automatically using QuickCheck \cite{QuickCheck}, which produces many similar test cases. In order to limit the number of tests they needed to consider, they partitioned the test set into equivalence classes.


\centerline{Given tests $T_O$ and $T_1$ and solution set $S$}
$$ T_0 \equiv T_1 \defined \forall s \in S : s(T_0) \equiv s(T_1) $$


Using this example, a test set $T$ could be reduced into a set of its equivalence classes. From these equivalence classes, a single representative example could be drawn, creating a new test set $T'$. $T'$ is a simpler set to work with than $T$, and processing it will be cheaper due to its decreased size. Most importantly, because $T'$ contains a test equivalent to every test in $T - T'$, the reduction has not lost any information.

%TODO: Proof by contradiction of this

Claessen's algorithm then creates a partial order of the solutions using $T'$. It does this using a binary representation of results. Tests can either be passed or failed, and PASS $>$ FAIL.

\centerline{Given solutions $S_1$ and $S_2$, and test set $T$}
$$S_1 \equiv S_2 \defined \forall t \in T : S_1(t) = S_2(t)$$
$$S_1 \sqsubseteq S_2 \defined \forall t \in T' : S_1(t) \geq S_2(t)$$


Under this definition, there are 4 possible results for comparing two solutions, $S_1$ and $S_2$. $S_1 < S_2$, $S_1 > S_2$, $S_1 \equiv S_2$, or $S_1$ and $S_2$  are incomparable. This definition allows us to build a partial ordering. This ranking is not built on the number of failures, but only on the direct relationships between the set of tests failed by each solution.

Using this algorithm, we are able to successfully create fair rankings of different student solutions.

\begin{figure}
\centering
\includegraphics{rank1.ps}
\caption{An edge points from a functionally inferior solution to a functionally superior one. "$\vert$" represents a class of tests passed by the equivalence class, "." represents a class of tests failed by the equivalence class.}
\end{figure}


\subsection{Adding More Outcomes}
The Claessen algorithm is based upon property based testing, for which the only possible outcomes are PASS and FAIL. Other forms of testing produce more outcomes. A solution can produce the right output, produce the wrong output, throw an exception, have a fatal error, or fail to terminate in a reasonable amount of time, among other possible outcomes.

Claessen's definitions already allow for non-binary outcomes. We need not change anything except what $>$ means. This step, however, presents an interesting problem. When allowing only binary outcomes, the total order of outcomes is self-evident: PASS$>$FAIL. When dealing with non-binary outcomes, however, there is no readily available ordering. Is an error that crashes the process ``better'' than one that silently produces wrong answers? In some cases, the process should be kept running at all costs. In others, the increased visibility of a catastrophic failure may be preferred to the subtle incorrectness of a wrong answer. In truth, the ranking of these outcomes is an arbitrary decision.

Since no universally correct ranking of outcomes is possible, we chose to examine what would happen if the algorithm was extended by a ranking determined by arbitrary choice. Though many orders could have been tried, for experimental purposes, we imposed a single simple arbitrary order. PASS$>$INCORRECT$>$FAILURE, where FAILURE is defined as an error that brings down the process/interpreter. We chose this ordering because our datasets represent student solutions, and the students were told that a segmentation fault (or related fatal error) would result in a 0\% as their grade on functional correctness.


\begin{figure}
\centering
\includegraphics[scale=0.75]{rank1.ps}
\caption{Created using binary outcomes}
\includegraphics[scale=0.75]{rank2.ps}
\caption{Created using multiple outcomes}
\end{figure}

Using our new comparison function (but an algorithm unaltered from Claessen's), we found several interesting properties. First, as demonstrated in Figures 2 and 3, in many cases, the additional outcomes made no difference. This was a surprising fact, and shows that the equivalence classes produced by the algorithm demonstrate a stronger degree of relatedness than previously known. Not only are these solutions getting the same binary results for every test, they are, in many cases, failing in  identical ways. Equivalence classes of solutions generally document programs that are \emph{extremely} similar.

In a small subset of our test cases
%TODO: quantify
we were able to improve the ranking using multiple outcomes. In most of these cases, it rendered incomparable solutions previously considered equivalent. This was due to them failing on the same tests, but in varying ways. In rare cases, we were able to rank solutions previously considered equivalent, generally because the solutions were identical, except on a subset of test on which one segfaulted and the other produced invalid output.

\subsection{Running Less Tests}

The computational power required to test a program adequately can be substantial. To fully exercise even a medium-scale program, thousands of unit tests (potentially generated from hundred of QuickCheck properties) may be required. Some or all of these tests may be expensive to run. A test that induces an infinite loop will run until an arbitrarily long timeout point. A test that does not induce an infinite loop may simply take a long time to run, or allocate large blocks of memory.

Running a test set can therefore be very expensive, particularly upon many solutions. Each expensive test needs to be run for each solution, and a particularly poor solution may cause many tests to loop infinitely. It would therefore be useful to not run the entire test set on every solution. 

For some of our test sets, we found that block testing was necessary for computational efficiency. For one of our assignments, we needed to test code in an interpreted environment. If we could not run tests in block, we needed to spawn an interpreter for each test. A test set created in this manner required ~4 hours of CPU time to run. Because test sets need to be debugged, this cost became prohibitively expensive.

Block testing, however, created its own set of problems. Rather than spawning a process for each test, we spawned a process for each block of related tests. If any of these tests brought down the entire process, results for any tests which were not yet reported were simply not reported. Instead, they received a new, implicit outcome: Did Not Run (DNR). Note that DNR means specifically that a test was not run at all. An outcome such as Did Not Finish (DNF) would appear explicitly in the test set.

With these new outcomes ``in'' our dataset, we were faced with the challenge of ranking based on them. There were two possible paths for ranking these solutions. The first would be to simply use the process described in the previous section, and extend the comparison function to place DNR arbitrarily within the ordering (perhaps as the worst result, or equivalent to a fatal error). This solution is simple, and allows us to reuse the modifications above. However, by giving DNR a place in the ordering, we limit what can cause a test to not run.

The tester running the test set may choose to omit tests from a given solution for any number of reasons. Perhaps tests are omitted for the reasons we used, because a previous test caused a fatal error. But perhaps thy are omitted for a more positive reason. A tester could choose to start a portion of a test set by running an extremely difficult test set, one which simultaneously exercised every feature that portion is testing. If this test is passed, they may then choose to not run all of the simpler test cases, because they know the solution will pass them. And perhaps the tester eliminates some tests for each of these reasons, on the same test. In this case, there is no proper place for DNR in the ordering of outcomes. It could mean either PASS or FAIL, and so cannot be placed as better, worse, or equivalent to any of them.

In this case, we can choose another method for handling DNR outcomes. The procedure is very simple.

\centerline{Given solutions $S_1$ and $S_2$, and test set $T$}
$$S_1 \equiv S_2 \defined \forall t \in T where S_1(t) \neq \dnr \wedge S_2(t) \neq dnr \wedge S_1(t) \equiv S_2(t)$$
$$S_1 > S_2 \defined \forall t \in T where S_1(t) \neq \dnr \wedge S_2(t) \neq dnr : S_1(t) \geq S_2(t)$$
In this way, we do not invalidate any of the possible meanings of DNR. In a properly formed test set, two solutions cannot differ only on tests for which one of them received a DNR. Instead, a prior test result must have \emph{caused} the DNR in one solution and not the other, and on this test they can still be compared. 

In the first situation previously discussed, in which a block of tests may not be run because a prior test crashed the block, solutions would differ on whether or not they ran the block, but the one which ran the block would also have an inherently better result---it passed the test which crashed the block. The solutions can be properly compared on this test.

In the second situation previously discussed, in wehich a block of tests may not be run because a prior test had a successful result, solutions would differ on whether they ran the block, but also on the initial test---the solution which did not run the block has an inherently better result---it passed the test which caused the block to not run. The solutions can be compared on this test.

Using this process, solutions can be compared whenever tests are not run, for whatever reason they are not run, as long as the reasons are deterministic. If tests are not run for a reason, we can still compare the solutions, based solely on the reason, \emph{without information loss}.

\section{The Implication Graph}
\subsection{Finding Relationships}
The methods described above treat each test as a distinct entity. They are used to compare solutions, but they are never compared to each other, and are treated as if they were independent. In real test sets, there are frequently subset of the tests which are not independent, but which instead build upon each other.

An example of this occurred when we tested student solutions to the problem of reducing terms in the lambda calculus. One feature we tested was whether students had properly implemented $\eta$-reduction. To test this, we enumerated terms with "holes" in them, and placed a subterm that should be eta-reduced within the hole. Two terms created are: 
$$T_1 : \lambda a.M_\eta a$$
$$T_2 : \lambda x.\lambda x.\lambda a.M_\eta a$$
$T_1$ tested an $\eta$-reduction on its own, with no other variables. We hypothesized that, if a student was to correctly reduce any other terms containing $\eta$-reductions, such as $T_2$, they would need to be able to successfully reduce $T_1$.

If our hypothesis is correct, there should be an underlying relationship in the data set. Failing $T_1$ should imply failing $T_2$. Additionally, failing $T_1$ should imply failing every other test that contains an $\eta$-reduction.

This hypothesis was known beforehand. However, the dataset contains many relationships similar to this implication. We have classified these relationships into four categories: real implications, accidental implications, trivial implications, and bogus implications. Real implications are those such as $T-1 \Rightarrow T_2$ above, which document an actual relationship between tests. Accidental implications are those which are represented in the data, but which have no bearing on the outer world, and are instead an "accident" of the solution population. Trivial implications include tautological implications and implications incidentally caused by a small piece of data, which are not representative of the entire solution population. Bogus relationships are those which are defined within the data, but which, given a sufficiently large population of solutions, could not possibly exist.

We create a structure we call the implication graph, which documents these implications. The simple implication graph includes every implication from all four classes. An improved implication graph attempts to eliminate as many trivial, bogus, and accidental implications as possible, while retaining the real implications.

\subsection{Creating the Graph}
An implication is defined as follows:

\centerline{Given solution set $S$ and outcomes $O_1$ on test $T_1$ and $O_2$ on test $T_2$}
$$O_1 \Rightarrow O_2 \iff \forall s \in S where s(T_1) = O_1, s(T_2) = O_2$$

Using this definition, we attempt to discover every implication in the dataset. First we create a proposition for every outcome, and its inverse, on every test. Then we look for an implication (in either direction) between each pair of propositions. This is a $O(n^2)$ process, but by limiting computation to one test from each equivalence class of tests, n remains small enough to proceed.

This process is simplified even further by usage of a variation of the partitioning algorithm used to create the equivalence classes of tests. Rather than partitioning the tests, we partition the \emph{propositions} into equivalence classes. For a given solution, a proposition is considered true if it holds on that solution (e.g. the proposition Test 2 PASS hold for a solution if it passes Test 2), and false if it does not hold. Two propositions, $P_1$ and $P_2$, are considered equivalent if they hold for the exact same set of solutions.
$$P_1 \equiv P_2 \iff \forall s \in S, P_1(s) = P_2(s)$$
Now, rather than having to find implications between every proposition, we need to find them between every \emph{class} of propositions.

The set of implications within a dataset forms a natural structure: a directed graph. Each class of propositions is a node in the graph, and each implication is an edge from the implicating proposition to the implied.

\begin{figure}
\verbatiminput{toyimpldata}
\includegraphics[width=\textwidth]{toyimpl.ps}
\caption{Dataset and the generated implication graph. A transitive reduction has been performed on the graph}
\end{figure}

Given this implication graph, the question remains: How many of the implications in the graph are real implications? We were provided with a natural experiment to begin to test this question. Our students were given an assignment that contained 14 problems. Each of the problems was entirely self-contained, and there was no code overlap between the problems. Some problems covered related topics, and so relic relationships might appear in the data, but by and large, we expected these problems to be diagrammed as entirely separate components.

\begin{figure}
\includegraphics[width=\textwidth]{div.ps}
\caption{Implication graph generated from experiment. Boxed sections indicate tests performed on the same problem. TODO: ADD BOXES}
\end{figure}

The results of this experiment were mixed. We did not find that every subproblem made an entirely separate subgraph, but instead that the subproblems made subgraphs that were sometimes connected, with implications between portions of the problem that should not be possible in the underlying problem.

\subsection{Improving the Implication Graph}

The implication graph contains every implication of all four classes (real, accidental, trivial, and bogus). An ideal graph contains every real implication, and no other implications. We have not been able to attain this graph in full, but there are several methods for limiting or eliminating the members of the negative three classes.

First, we can readily remove two types of bogus implications. An implication is bogus if exists in the implication graph, but provably cannot exist in the full problem space. The two types of implications we can remove are any implications of the form $\pass\Rightarrow\fail$ or $\fail\Rightarrow\pass$.

Assumption: There exists a fully correct and fully incorrect solution to every problem.


\begin{thm} $\pass(T_1)$ cannot imply $\fail(T_2)$

\begin{proof} Assume $\pass(T_1) \Rightarrow \fail(T_2)$. This means that every solution that passes $T_1$ fails $T_2$. However, there exists the fully correct solution S. For all tests T, S(T) = PASS. Therefore S($T_1$) = PASS, and S($T_2$) = PASS, contradicting our assumption.
\end{proof}
\end{thm}

\begin{thm} $\fail(T_2)$ cannot imply $\pass(T_1)$

\begin{proof} Assume $\fail(T_1) \Rightarrow \pass(T_2)$. This means that every solution that fails $T_1$ passess $T_2$. However, there exists the fully incorrect solution S'. For all tests T, S'(T) = FAIL. Therefore S'($T_1$) = FAIL, and S'($T_2$) = FAIL, contradicting our assumption.
\end{proof}
\end{thm}



Based on these proofs, we can safely remove from the implication graph every implication between the passage of one test and the failure of another. This can be done in one of two ways. Either we can look for the implication graph and remove these implications where they exist, or we can create it with a fully correct and a fully incorrect solution as members of the solution set.

Introducing the fully correct and fully incorrect solutions will also remove a class of trivial implications: implications created from tests with uniform outcomes. If every solution gets the same outcome on a given test, this outcome will be trivially implied by every outcome of every test. Similarly, if no student gets an outcome on a given test, this outcome trivially implies every outcome of every test. These implications add no useful information, and can either be removed explicitly, or implicitly by the introduction of the fully correct and incorrect solutions.

There is a related type of trivial implications we can remove, though this one must be removed explicitly. If a particular outcome is achieved by exactly one equivalence class of solutions, this outcome will trivially imply every other outcome achieved by this student. If solution S is the only student to fail test $T_1$, and also fails $T_2$ and $T_3$, $\fail(T_1) \Rightarrow \fail(T_2)$ and $\fail(T_1) \Rightarrow \fail(T_2)$ regardless of the outcomes of every other solutions. This is because every solution that failed $T_1$ (only S) also achieved every other outcome S achieved. 

Removing this set of implications is particularly important, because they are highly misleading. They make the claim that, if you know one thing about a solution, you can know everything about it, which is untrue. They are not classified as bogus, however, because the implication may actually be real. We simply don't have enough information to know either way.

We can also easily remove another type of trivial implication: tautologies. A tautology exists when an implication will hold true regardless of the dataset, for example : $\pass(T_1) \Rightarrow \neg\dnr(T_1)$ and $\dnr(T_1) \Rightarrow \neg\fail(T_1)$. These implications can simply be removed, because they do not add any useful information to the set of implications.

These reductions are able to improve the ratio between real implications and the other classes, but they are not perfect. Notably, we do not have a way to differentiate between real and accidental implications. These methods improve the quality of the implication graph, but there is considerable room for growth.

\section{Noise Reduction}

\subsection{What is Noise?}
A piece of data is classified as noise if it can be removed without causing any loss of information. For example, in Figure {?} we show that the number of tests run is generally far greater than the number of equivalence classes they form. However, for our ranking algorithm, we only need a single representative from each equivalence class to create our ranking. The non-representative members of each equivalence class are noise. 

Noise can be found in any section of a data set. It can be entire tests, as above, or single outcomes. In either case, its presence in the final output of our program is a negative force. The more noise there is in the final data, the less likely it is for the reader, in our case a student, to discover the useful information.

For this reason, we seek to reduce noise as much as possible. Ideal noise reduction algorithms would provide each student the minimum subset of testing data required to accurately compare themself with other students, and properly diagnose the faults within their program.


\subsection{Claessen's Algorithm}
In Claessen's paper on ranking, there are several forms of noise reduction already performed. The first is the fundamental step of the algorithm, in which tests are grouped into equivalence classes. This step alone reduces from the original number of tests to a substantially smaller subset (see Table 1).

After this large scale reduction, Claessen's algorithm contains another form of noise reduction. A test $T_0$ is redundant if

$$\fail (T_0) \equiv fail(T_1) \cup \fail(T_2)$$
where all three tests are drawn from different equivalence classes. This was justified with the argument that, rather than discovering a new faults of its own, $T_0$ likely provoked both of the faults discovered in $T_1$ and $T_2$. Removing $T_0$ therefore does not cause any information loss. Instead, its removal simplifies the test set to a smaller, but still representative sample of tests.

\subsection{Union*}
The logical basis to Claessen's algorithm that a test can be redundant because it provokes the flaws already demonstrated by two other non-redundant tests\cite{Claessen}, can be extended further. Why should we limit this to only circumstances where the flaws are in exactly two other tests? Why not any number? We propose an extended algorithm, one which removes any tests whose set of failures is equal to the union of the failures of \emph{any number of tests}. 

A test $T_0$ is redundant if

$$\fail (T_0) \equiv \fail(T_1) \cup \fail(T_2) \cup ... \cup \fail(T_n) $$

(if I managed to write the first proof, proving this the same shouldn't be hard)

A naive implementation of this algorithm is computationally complex. Given a set of tests $T$, we need to run the algorithm for each member $t$ of $T$, checking whether its failure set is equal to the union of any subset of tests in $T$ that does not contain $t$. This is simply too much work, so a different solution must be found.

Thankfully, the implication graph lends itself to a vastly simplified implementation. We do not need to check the union of every possible subset of tests, only the union of those that could possibly be useful. Using the implication graph, given a test $t$ and a test set $T$, we can easily find the subset $T'$ which only contains tests whose failures are a subset of $t$. Any test in $T$ but not in $T'$ cannot possibly contribute to an equal union, because it contains at least one failure not in $t$. Therefore, we can simply apply the algorithm once, to every member of $T'$. $t$ is redundant if $\fail(t) \equiv \cup^* T'$.

\subsection{Comparison}
Union* must reduce at least as many tests as Claessen, and can reduce more. This is because every test that will be removed under Claessen will be removed under exactly the same conditions in Union* (the union of two tests is simply a specification of the union of any number of tests). In theory, Union* is capable of outperforming Claessen

\begin{figure}
\verbatiminput{testset}
\includegraphics{toy.ps}
\caption{Reduced under Claessen. t = {1,2,3,4}}
\includegraphics{toyb.ps}
\caption{Reduced under Union*. t = {1,2,3}}
\end{figure}

%TODO: Table with different levels of reductions. Then change the text below to handle whatever I put in the table.

In practice, results were mixed. Out of 21 datasets testes, Claessen and Union* performed identically 18 times. In the 3 datasets reduced, it reduced an average of 3 tests per test set, a 5\% reduction.

\begin{table}[t]
\centering
\begin{tabular}{ | l | l | l | l | l | }
\hline
Test & Test Count & Equivalence Classes &  Claessen Reduction &  Union* Reduction \\ 
\hline
105 & 135 & 8 & 8 & 8 \\
all & 20 & 10 & 8 & 8 \\
asm & 96 & 27 & 19 & 18 \\
asmcoding & 135 & 6 & 5 & 5 \\
bitpack & 2468 & 144 & 91 & 87 \\
comp40intro & 20 & 12 & 10 & 10 \\
divtest & 2112 & 71 & 40 & 40 \\
eq & 6 & 4 & 4 & 4 \\
extras & 45 & 9 & 7 & 7 \\
inf & 96 & 71 & 46 & 42 \\
linterp & 435 & 31 & 20 & 20 \\
nr & 22 & 10 & 10 & 10 \\
pred & 6 & 3 & 3 & 3 \\
required & 54 & 11 & 9 & 9 \\
size & 180 & 26 & 19 & 19 \\
sudoku & 19 & 6 & 6 & 6 \\
tuscheme & 96 & 49 & 38 & 38 \\
typesys & 85 & 41 & 33 & 33 \\
u & 256 & 53 & 29 & 29 \\
unblack & 23 & 14 & 14 & 14 \\
unify & 256 & 53 & 29 & 29 \\
\hline
\end{tabular}
\caption{The results of each reduction algorithm}
\end{table}

There are several potential explanations for the fact that Claessen and Union* generally perform identically. One is that no test tests more than two underlying faults. This is extremely unlikely, as it is relatively simple to write a test that exercises large sections of a program's functionality, and can readily provoke more than two potential faults. Another possibility lies in the method of building tests. When unit tests are written, they generally exercise small areas of functionality, rather than the entire program. A different style of test design might allow Union* to have more success, removing large tests that attempt to exercise every possible fault in the program, when these faults are better demonstrated by small tests. A final possibility is that we simply don't have enough solutions, and that a greater diversity in solutions would increase the difference between the two algorithms.




\section{Reducing Noise in Witness Sets}
\subsection{Witness Sets}
When grading students, successfully ranking them is not the only goal of creating and running a test set. An additional goal is providing the student with feedback on the flaws within their program. We do this programmatically by providing them their test results on each test which they failed, as well as a witness describing what went wrong.

This witness set is drawn only from the tests which are used to rank the student solutions. Thus, the noise reduction strategies above implicitly reduce noise in the witness set, by eliminating tests from consideration. However, this still leaves many tests in the witness set, and we believe many of them can be classified as noise. If this noise can be reduced, students can be provided a simpler set of witnesses, making it easier for them to understand the flaws in their programs.

\subsection{The Algorithm}
Any reductions that can be made on the basis of classes of solutions will already be implemented as reductions within the ranking graph. However, when attempting noise reduction at the granularity of a single solution, the relationships between solutions are not applicable. The only relationships relevant at this level  are those between the tests themselves.

The implication graph is a representation of the relationships between these tests. It presents many discoverable relationships (implies, implied by, reachable from, etc.). Using this graph, we are able to perform noise reduction at the level of a single solution, a novel process. For simplicity, we chose to examine only one of the available relationships, direct implication. Our algorithm is as follows:

\centerline{Given two tests $T_1$ and $T_2$ and solution $s$}
$$if s(T_1) = \fail \wedge \fail(T_1) \Rightarrow \fail(T_2), remove T_2$$

Given a perfect implication graph, in which every implication is not just found in the data but representative of the problem itself, this algorithm reduces the tests presented as  failed to the set of the easiest tests failed. 

If two tests are needed to differentiate the solutions on the solution class scale, it is possible (and frequently true), that one is simply an easier version of itself. For example, take the lambda terms described above when creating the implication graph. 

$$T_1 : \lambda a.M_\eta a$$
$$T_2 : \lambda x.\lambda x.\lambda a.M_\eta a$$

Because the flaw in $T_1$ directly causes a failure in $T_2$, $\fail(T_1) \Rightarrow \fail(T_2)$. This led our algorithm to be able to reduce the witnesses to any solution who failed both tests, providing only the output from $T_1$.

\begin{figure}
\verbatiminput{lambdareduction}
\caption{Machine output for a successful reduction in which $T_2$ (nr403) was removed due to failure of $T_1$ (nr97)}
\end{figure}

\subsection{Validity of Reductions}

This algorithm is definitively  capable of performing reductions, but a questions remains: Are the reductions good? Both the Claessen algorithm and the Union* algorithm do not cause information loss. But the witness reductions can. Removal based on real implications does not cause information loss - it removes uninformative tests. But removal based on accidental, bogus, or trivial implications can cause information loss, by removing witnesses that in fact provide useful information.

We discovered that the above reductions were critical in producing a val set of reductions.

A readily available example is the dataset used to confirm the implication graph. As we discovered earlier, when tests were run on subproblems whose implementations were entirely independent, the subproblems generally formed into groups in the  graph, but there were implications between the groups. We assumed that the implications within this graph were a mixture of real and accidental: implications within subproblems were likely real, implications between subproblems were likely accidental. 

\begin{comment}
\begin{figure}
\verbatiminput{goodreductions}
\caption{Valid reductions.}
\verbatiminput{badreductions}
\caption{Invalid reductions.}
\end{figure}
\end{comment}

\begin{figure}
\verbatiminput{example.witness}
\caption{Unreduced output}
\verbatiminput{example3.witness}
\caption{Reduced with unreduced implication graph}
\verbatiminput{example2.witness}
\caption{Reduced using improved implication graph}
\caption{In the student's code, none of the subproblemss call each other. Therefore any implication between subproblems was accidental based on the code base.}
\end{figure}

The naive implication graph created from this problem had many implications that could be reduced. We found that applying these reductions had a significant effect. Witness sets reduced using the reduced implication graph were larger than those produced on the naive one. The additional tests contained were tests that were incorrectly reduced based on invalid implications.

Using the reduced implication graph, we performed a witness reduction on each solution. We did this to test how many of the reductions we considered to be valid, and based upon real implications. In order to simplify analysis, we did not try to delve into the relationship between each test, but instead used a simple decision process. If two tests test the same subproblem, any reduction upon them is deemed valid. If they test different functions, any reduction on them is deemed invalid.

%\begin{figure*}
%\begin{tabularx}{\linewidth}{@{}XX@{}}
%\small\verbatiminput{goodreductions}
%&
%\small\verbatiminput{badreductions}
%\\
%\caption{Good reductions}&
%\caption{Bad reductions}
%\\
%\end{tabularx}
%\end{figure*}


After running this experiment, we found that the witness-reduction strategy achieved moderate success. Out of 422 reductions, 62.5\% were deemed to be valid. However, this result is somewhat misleading. A large number of the invalid reductions were concentrated in the least correct solutions. By omitting the reductions obtained from the two least correct solutions, the percentage of correct reductions jumps to 76.7\%. While eliminating these students is not something that can truly be done in practice, it demonstrates that witness reduction is highly accurate for most students, and extremely inaccurate for a select few.

\subsection{The Results  of Improving the Implication Graph}

These reductions of the implication graph are critical to our ability to implement witness reduction. The first several reductions insured we were not making any reductions based on clearly invalid implications. The final reduction, in which we do not allow implications based on unique outcomes, was particularly important. Based on this reduction, we were able to improve the output and give larger and more representative witness sets to the affected solutions. These sets are not 

\begin{figure}
\verbatiminput{example.witness}
\caption{Unreduced output}
\verbatiminput{example3.witness}
\caption{Reduced with unreduced implication graph}
\verbatiminput{example2.witness}
\caption{Reduced using improved implication graph}
\caption{In the student's code, none of the functions call each other. Therefore the reduction across tests is not only invalid under our metric, but accidental based on the code base.}
\end{figure}

\section{Future Work}
\subsection{Witness Expansion}

In this paper, we use the implication graph for the purpose of witness reduction, removing noisy information from the witness set. However, the implication graph may potentially be usable to \emph{create} information, providing more accurate diagnostic information by \emph{expanding} the witness set. 

When we believed two tests were related based on the implication graph, we removed the harder test, because the simpler one provided the diagnostic information. However, for a more successful solution, which passed the easier test but failed the harder one, we provide no evidence of this relationship. But this relationship is a very useful thing, because it provides increased specificity on exactly where the fault lies within the solution. Using a very similar formula to our witness reduction, we can provide the student with these relationships.

\centerline{We inform a solution $S$ which passed test $T_1$ but failed $T_2$ of the results on  $T_1$ iff}
$$ S(T_1) = PASS \wedge S(T_2) = \fail \wedge \fail(T_1) \Rightarrow \fail(T_2)$$

\subsection{Additional Reduction Algorithms}


This paper utilized the implication graph in order to implement several forms of noise reduction. These are not, however, every form of noise reduction the graph can be used for. By documenting the structure of a problem, the implication graph makes it easy for a researcher to implement new reduction algorithms. For example, rather than removing tests whose failures are the union of the implying failures, we could remove any test whose failures are the intersection of the implied failures. Or any test where every outcome is implied by another test. There are many possible reduction algorithms, and with the infrastructure of the implication graph in place, these algorithms can be readily implemented.

\section{Conclusion}
Already in the literature are algorithms to rank programs based on black-box testing results. These algorithms are successful, but they use only a subset of the information available. We have attempted to use as much of the information present as possible to improve the existing algorithms and create our own.

Our Union* algorithm uses more of the testing data available to build a generalization of the union based reduction strategy implemented in \cite{Claessen}. This strategy proved to have minimal effect on the number of reduced tests in practice, but is computationally simpler than the Claessen algorithm, once the infrastructure has been built.

Our witness reduction algorithm is an entirely novel form of noise reduction. Rather than reducing tests only for the entire set of solutions, this algorithm allows further reductions to take place for each individual solution, creating a smaller set of failed test cases from which the solution's creator can diagnose faults.

Most importantly, we have created a powerful infrastructure from which further methods of noise reduction can easily be implemented. Rather than having to redesign for each new algorithm implemented, the implication graph allows for new reduction algorithms to be quickly implemented, and for entirely new types of reductions to be tested.

\bibliographystyle{plainnat}
\bibliography{thesis}

\end{document}
